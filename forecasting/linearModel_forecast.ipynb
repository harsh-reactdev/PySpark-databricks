{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ff4397-3c68-4cbe-923c-fc7e80cc3d7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "imports"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, col, to_timestamp, min as spark_min, unix_timestamp, expr, from_unixtime\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, LongType, IntegerType,\n",
    "    BooleanType, ArrayType, TimestampType, DoubleType\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6b7bf962-690e-42b2-805f-ec6c439c8d5f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "schema and data loading"
    }
   },
   "outputs": [],
   "source": [
    "toner_usage_schema = StructType([\n",
    "\n",
    "    StructField(\"deviceId\", StringType(), True),\n",
    "    StructField(\"dealerId\", StringType(), True),\n",
    "    StructField(\"L1DealerId\", StringType(), True),\n",
    "    StructField(\"L2DealerId\", StringType(), True),\n",
    "    StructField(\"L3DealerId\", StringType(), True),\n",
    "    StructField(\"divisionId\", StringType(), True),\n",
    "    StructField(\"servicingDealerId\", StringType(), True),\n",
    "    StructField(\"customerId\", StringType(), True),\n",
    "\n",
    "    StructField(\"modelName\", StringType(), True),\n",
    "    StructField(\"serialNumber\", StringType(), True),\n",
    "\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"lastSuppliesUpdate\", StringType(), True),\n",
    "\n",
    "    StructField(\"relatedGroupId\", StringType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"typical\", StringType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"capacity\", StringType(), True),\n",
    "\n",
    "    StructField(\"printCount\", IntegerType(), True),\n",
    "    StructField(\"tonerNumber\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"tagIds\", StringType(), True),\n",
    "    StructField(\"forecastingList\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format('csv').schema(toner_usage_schema).option('header', True).load('/Volumes/workspace/default/forecast/Toner Usage.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa8a51b-f085-410f-9bd4-d15019568179",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "base data prep"
    }
   },
   "outputs": [],
   "source": [
    "df_base = (\n",
    "    df\n",
    "    .select(\n",
    "        col(\"deviceId\"),\n",
    "        col(\"color\"),\n",
    "        to_timestamp(col(\"timestamp\")).alias(\"timestamp\"),\n",
    "        col(\"typical\").cast(\"double\").alias(\"typical\"),\n",
    "        col(\"printCount\").cast(\"int\").alias(\"printCount\")\n",
    "    )\n",
    "    .dropna(subset=[\"deviceId\", \"color\", \"timestamp\", \"typical\"])\n",
    ")\n",
    "\n",
    "df_base = df_base.filter(col(\"typical\") > 0)\n",
    "\n",
    "df_base.orderBy(\"deviceId\", \"color\", \"timestamp\").show(10, truncate=False)\n",
    "df_base.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d1999258-a495-40d2-a01e-8e05b058c205",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767775772632}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "timestamp to minutes"
    }
   },
   "outputs": [],
   "source": [
    "w = Window.partitionBy(\"deviceId\", \"color\")\n",
    "\n",
    "df_time = df_base \\\n",
    "    .withColumn(\"t0\", spark_min(\"timestamp\").over(w)) \\\n",
    "    .withColumn(\n",
    "        \"minutesSinceStart\",\n",
    "        (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"t0\"))) / 60\n",
    "    )\n",
    "\n",
    "df_time.select(\n",
    "    \"deviceId\",\n",
    "    \"color\",\n",
    "    \"timestamp\",\n",
    "    \"minutesSinceStart\",\n",
    "    \"typical\",\n",
    "    \"printCount\"\n",
    ").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28d169cf-9baf-423d-94f1-9494e9319f3e",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767775814163}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "select regression columns"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df_lr = df_time.select(\n",
    "    \"deviceId\",\n",
    "    \"color\",\n",
    "    \"minutesSinceStart\",\n",
    "    \"typical\",\n",
    "    \"printCount\",\n",
    "    \"t0\"\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"typical\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_lr = assembler.transform(df_lr)\n",
    "\n",
    "df_lr.select(\n",
    "    \"deviceId\",\n",
    "    \"color\",\n",
    "    \"minutesSinceStart\",\n",
    "    \"features\",\n",
    "    \"typical\"\n",
    ").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "11b930c6-0472-4746-bade-b53dfcf0d7a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "function to fit per group"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"deviceId\", StringType()),\n",
    "    StructField(\"color\", StringType()),\n",
    "    StructField(\"slope\", DoubleType()),\n",
    "    StructField(\"intercept\", DoubleType())\n",
    "])\n",
    "\n",
    "def forecast_toner(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    if len(pdf) < 2:\n",
    "        return pd.DataFrame([], columns=[\"deviceId\", \"color\", \"slope\", \"intercept\"])\n",
    "    X = pdf[[\"minutesSinceStart\"]].values\n",
    "    y = pdf[\"typical\"].values\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    return pd.DataFrame([{\n",
    "        \"deviceId\": pdf[\"deviceId\"].iloc[0],\n",
    "        \"color\": pdf[\"color\"].iloc[0],\n",
    "        \"slope\": lr.coef_[0],\n",
    "        \"intercept\": lr.intercept_\n",
    "    }])\n",
    "\n",
    "final_predictions_df = (\n",
    "    df_lr\n",
    "    .groupBy(\"deviceId\", \"color\")\n",
    "    .applyInPandas(forecast_toner, schema)\n",
    "    .sort(\"deviceId\", \"color\")\n",
    ")\n",
    "\n",
    "display(final_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "48e45104-ce50-466e-b9f9-d236789949ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fit the model per device and per color"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Add a column for predicted minutes until typical reaches zero\n",
    "final_predictions_df = final_predictions_df.withColumn(\n",
    "    \"minutesUntilDepletion\",\n",
    "    -final_predictions_df[\"intercept\"] / final_predictions_df[\"slope\"]\n",
    ")\n",
    "\n",
    "display(final_predictions_df.select(\n",
    "    \"deviceId\",\n",
    "    \"color\",\n",
    "    \"slope\",\n",
    "    \"intercept\",\n",
    "    \"minutesUntilDepletion\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b276c5fc-bde3-4f3a-b3eb-71fdd8903343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min as spark_min, unix_timestamp, from_unixtime\n",
    "\n",
    "# Replace \"created_at\" with your actual timestamp column name\n",
    "df_start = (\n",
    "    df_lr\n",
    "    .groupBy(\"deviceId\", \"color\")\n",
    "    .agg(\n",
    "        spark_min(\"t0\").alias(\"startTimestamp\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_forecast = (\n",
    "    final_predictions_df\n",
    "    .join(df_start, [\"deviceId\", \"color\"], \"inner\")\n",
    "    .withColumn(\n",
    "        \"predictedDepletionTimestamp\",\n",
    "        from_unixtime(\n",
    "            unix_timestamp(col(\"startTimestamp\")) + col(\"minutesUntilDepletion\") * 60\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_forecast.select(\n",
    "    \"deviceId\",\n",
    "    \"color\",\n",
    "    \"minutesUntilDepletion\",\n",
    "    \"predictedDepletionTimestamp\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c401834-0f7e-403b-a824-3605aa2e6c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Schema for regression parameters\n",
    "printcount_schema = StructType([\n",
    "    StructField(\"deviceId\", StringType()),\n",
    "    StructField(\"color\", StringType()),\n",
    "    StructField(\"slope\", DoubleType()),\n",
    "    StructField(\"intercept\", DoubleType())\n",
    "])\n",
    "\n",
    "def forecast_printcount(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    if len(pdf) < 2:\n",
    "        return pd.DataFrame([], columns=[\"deviceId\", \"color\", \"slope\", \"intercept\"])\n",
    "    X = pdf[[\"minutesSinceStart\"]].values\n",
    "    y = pdf[\"printCount\"].values\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X, y)\n",
    "    return pd.DataFrame([{\n",
    "        \"deviceId\": pdf[\"deviceId\"].iloc[0],\n",
    "        \"color\": pdf[\"color\"].iloc[0],\n",
    "        \"slope\": lr.coef_[0],\n",
    "        \"intercept\": lr.intercept_\n",
    "    }])\n",
    "\n",
    "# Fit regression per device and color\n",
    "final_printcount_df = (\n",
    "    df_lr\n",
    "    .groupBy(\"deviceId\", \"color\")\n",
    "    .applyInPandas(forecast_printcount, printcount_schema)\n",
    "    .sort(\"deviceId\", \"color\")\n",
    ")\n",
    "\n",
    "display(final_printcount_df)\n",
    "\n",
    "# Join with final_predictions_df to get minutesUntilDepletion\n",
    "df_remaining = (\n",
    "    final_printcount_df\n",
    "    .join(final_predictions_df.select(\"deviceId\", \"color\", \"minutesUntilDepletion\"), [\"deviceId\", \"color\"], \"inner\")\n",
    "    .withColumn(\n",
    "        \"predictedRemainingPrintCount\",\n",
    "        final_printcount_df[\"intercept\"] + final_printcount_df[\"slope\"] * col(\"minutesUntilDepletion\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_remaining.select(\n",
    "    \"deviceId\",\n",
    "    \"color\",\n",
    "    \"minutesUntilDepletion\",\n",
    "    \"predictedRemainingPrintCount\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea52c16-a7a9-4ff0-a651-e43fc1877809",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767779754083}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_8c63d75f\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_2976a335\",\"enabled\":true,\"columnId\":\"deviceId\",\"dataType\":\"string\",\"filterType\":\"eq\",\"filterValue\":\"mn=QlA1MEM2NQ==:sn=NDMwMTE1MjgwMA==\",\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1767782125052}],\"syncTimestamp\":1767782125052}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"like\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"deviceId\"},{\"kind\":\"literal\",\"value\":\"mn=QlA1MEM2NQ==:sn=NDMwMTE1MjgwMA==\",\"type\":\"string\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "final_table = (\n",
    "    df_base\n",
    "    .join(\n",
    "        df_forecast.select(\n",
    "            \"deviceId\",\n",
    "            \"color\",\n",
    "            \"predictedDepletionTimestamp\"\n",
    "        ),\n",
    "        [\"deviceId\", \"color\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_remaining.select(\n",
    "            \"deviceId\",\n",
    "            \"color\",\n",
    "            \"predictedRemainingPrintCount\"\n",
    "        ),\n",
    "        [\"deviceId\", \"color\"],\n",
    "        \"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"deviceId\", \"color\").orderBy(col(\"timestamp\").desc())\n",
    "\n",
    "final_table_one_row = (\n",
    "    final_table\n",
    "    .withColumn(\"rn\", row_number().over(w))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "display(\n",
    "    final_table_one_row.select(\n",
    "        \"deviceId\",\n",
    "        \"timestamp\",\n",
    "        \"color\",\n",
    "        \"typical\",\n",
    "        \"printCount\",\n",
    "        \"predictedDepletionTimestamp\",\n",
    "        \"predictedRemainingPrintCount\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "linearModel_forecast",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
